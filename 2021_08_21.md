# 얕은 신경망에 대해서 공부.

## 1. 신경세포 와 인공신경망.

딥러닝에서 신경망은 생물학적 신경세포를 단순화 하였다. 
- 생물학적 신경세포 : 여러 신호를 받아, 하나의 신호를 만들어 전달.
- 딥러닝 신경망 : 여러 입력 값을 받아, 하나의 값을 전달. 이를 뉴런이라고 한다.

딥러닝 신경망은 출력을 내기전에 활성 함수 (Activation Function)을 통해 비선형을 가할 수 있다. 
![뉴런](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile25.uf.tistory.com%2Fimage%2F99EAB2505AE487821C8A8F)
<center> [그림 1.1] 생물학적 신경 세포(좌)와 딥러닝 신경망(우) </center> 

- 인공 신경망(Artificial Neural Netword) : 뉴런이 모여 서로 연결된 형태.

앞으로 모든 딥러닝 네트워크는 인공 신경망을 기반으로 하고 있다. 

## 2. 얕은 신경망

 - 전결합 계층(Fully-Connected Layer 또는 Dense Layer): 이전 계층(layer)과 모든 뉴런이 서로 연결된 계층

얕은 신경망(Shallow Neural Netword)은 입력, 은닉, 출력 3가지 계층으로 되어있으며, 은닉 계층과 출력 계층이 Fully-Connected Layer 계층인 모델이다.
![얕은 신경망](https://heung-bae-lee.github.io/image/ShallowNN.png)
<center> [그림 1.2] 얕은 신경망 </center> 

이러한 얕은 신경망을 이용하여 무엇을 할 수 있을까? 
- 입력과 출력 간의 상관성이 있을 경우, 이를 학습하여 새로운 입력에 대해 출력을 낼 수 있다. 
```
ex.) 
> 입력 : 키, 몸무게, 나이      --> 출력 : 기대 수명
> 입력 : 집, 지역, 건축 년도 등 --> 출력 : 적정 매매가
> 입력 : 면접 점수, 실기 점수   --> 출력 : 당락 여부
> 입력 : 꽃잎의 너비, 색깔     --> 출력 : 꽃의 종류
```

## 3. 회귀와 분류

- 회귀(regression) : 잡음이 있는 학습 샘플로부터 규칙을 찾아 연속된 값의 출력을 추정하는 것.
- 분류(classification) : 입력 값을 분석해 특정 카테고리로 구분하는 작업
  - 카테고리가 2개 : 이진 분류 (binary classification)
  - 카테고리가 3개 이상 : 다중 분류 (multi-class classification)

## 4. 얕은 신경망을 이용한

- 회귀: 얕은 신경망의 동작은 출력 계층의 활성함수에 의해 달라진다. 회귀는 전 범위의 연속된 값을 출력하므로, 보통 항등함수를 사용한다. 
- 이진 분류: 출력은 0\~1 사이의 실수 값. Sigmoid function을 사용 
  - 분류 방법: 0.5보다 작으면 첫번째 class, 크면 두번째 class
![sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png)
<center> [그림 1.3] sigmoid function </center> 


- 다중 클래스 분류: SoftMax 활성 함수를 이용. 각 출력은 해당 class에 속할 확률을 의미한다. 

## 5. 수학적으로 접근.

뉴런을 수식으로 표현하면 다음과 같다. 

$$
y = a(\sum^{N-1}_{i=0}w_ix_i + b)
$$

- $x_i$ : 입력 값
- $w_i$ : 가중치 값
- $b$ : 편향 (bias)

> 편향이란?
> 
> 편향은 하나의 뉴런으로 입력된 모든 값을 다 더한 다음에 이 값에 더해주는 상수이다. 이 값은 하나의 뉴런에서 활성하 함수를 거쳐 최종적으로 출력되는 값을 조절하는 역할이다. 

여기서 우리는 $w, x$를 다음과 같이 정의하자. 

$$
\textbf{w} := 
\begin{bmatrix}
    w_0\\
    w_1\\
    \vdots\\
    w_{N-1}
\end{bmatrix}, \qquad
\textbf{x} := 
\begin{bmatrix}
    x_0\\
    x_1\\
    \vdots\\
    x_{N-1}
\end{bmatrix}
$$

뉴런의 수식을 두 벡터의 내적으로 표현할 수 있다.
$$
\begin{aligned}
    y = a(\textbf{w}^{T}\textbf{x} + b)
\end{aligned}
$$

### Fully connected layer의 수학적 표현

Fully connected layer의 각 출력 노드를 수식으로 표현하면 다음과 같다. 

$$
\begin{aligned}
    y_0 &= a(\textbf{w}_0^{T}\textbf{x} + b_0)\\
    y_1 &= a(\textbf{w}_1^{T}\textbf{x} + b_1)\\
    y_2 &= a(\textbf{w}_2^{T}\textbf{x} + b_2)\\
    \vdots\\
    y_{M-1} &= a(\textbf{w}_{M-1}^{T}\textbf{x} + b_{M-1})\\
\end{aligned}
$$

$\textbf{y}, \textbf{W}, \textbf{b}$를 다음과 같이 정의하자.
$$
\textbf{y} := 
\begin{bmatrix}
    y_0\\
    y_1\\
    \vdots\\
    y_{M-1}
\end{bmatrix} \qquad
, 

\textbf{W} :=
\begin{bmatrix}
    \textbf{w}_0\\
    \textbf{w}_1\\
    \vdots\\
    \textbf{w}_{M-1}\\
\end{bmatrix} \qquad
, 
\textbf{b} :=
\begin{bmatrix}
    b_0\\
    b_1\\
    \vdots\\
    b_{M-1}
\end{bmatrix}
$$

Fully connected layer를 행렬 곱셈 연산으로 표현할 수 있다. 
$$
\textbf{y} = a(\textbf{W}\textbf{x} + \textbf{b})
$$

## Fully connected layer의 각 계층 특징

- 입력 계층 (input layer)
    - 아무런 연산도 일어나지 않음.
    - 신경망의 입력을 받아서 다음 계층으로 넘기는 역할.
    - 어떤 데이터를 입력해주어야 하는가에 대한 문제가 있다. (특징 추출 문제)
      - 이러한 특징 추출 문제는 직접 해주어야 한다.
    - 계층의 크기 = 노드의 개수 = 입력 스칼라 수 = 입력 벡터 길이

$$
\textbf{x} := 
\begin{bmatrix}
    x_0\\
    x_1\\
    \vdots\\
    x_{N-1}
\end{bmatrix}
$$

- 은닉 계층 (hidden layer)
  - 입력 계층과 연결된 fully connected layer이다. 
  - 입출력 관점에서 볼 때 드러내지 않는다고 하여 은닉 계층이라 한다. 
  - 복잡한 문제를 해결할 수 있게 하는 핵짐적인 계층
  - 얕은 신경망에서는 1개의 은닉 계층만을 사용한다. 

$$
\textbf{h} = a_h(\textbf{W}_h\textbf{x} + \textbf{b}_h)
$$

- 출력 계층 (output layer)
  - 은닉 계층 다음에 오는 fully connected layer이다.
  - 신경망의 외부로 출력 신호를 전달하는데 사용.
  - 신경망의 기능은 출력 계층의 활성 함수에 의해 결정
  - 출력 계층의 크기 = 출력의 스칼라 수 = 출력 벡터의 길이.

$$
\textbf{y}_{o} = a_{o}(\textbf{W}_{o}\textbf{h} + \textbf{b}_{o})
$$

### 회귀를 수학적으로 이해

회귀는 다음 두가지로 나뉘어 진다. 
- 단순 선형 회귀
- 다중 선형 회귀

#### 단순 선형 회귀

- 단순 선형 회귀 : 데이터를 가장 잘 표현하는 선형식을 찾는 동작. 
![linear](https://img1.daumcdn.net/thumb/R720x0.q80/?scode=mtistory2&fname=http%3A%2F%2Fcfile7.uf.tistory.com%2Fimage%2F997E924F5CDBC1A6283C93)
<center> [그림 1.4] 선형 회귀</center>
위 그림에서 추정한 예측값의 식은 다음과 같다. 

$$
y = wx + b
$$

선형 회귀는 위 식의 $w, b$를 찾는 것이다. 가장 최적의 $w$ 를 찾는 식은 다음과 같다. 

$$
w^* = \text{arg}\underset{w}{\text{ min}} \frac{1}{N}\{\sum_i \frac{(y-y_i)^2}{2}\}
$$
위 식은 평균제곱에러(MSE)를 최소로 하는 $w$를 찾기위한 식이다.

#### 다중 선형 회귀

다중 선형 회귀의 수식은 다음과 같다. 
$$
\begin{aligned}
     y &= w_0x_0 + w_1x_1 + \cdots + w_{N-1}x_{N-1} + b\\
  &= \sum^{N-1}_{i=0}w_ix_i + b\\
  &= \textbf{w}^{T}\textbf{x} + b
\end{aligned}
$$
단일 입력이 아닌, 다중 입력을 받을 경우에는 변수가 확장되어 벡터의 내적이 된다. 

이러한 다중 선형 회귀를 기하학적 해석은 다음과 같다. 
> 변수가 하나 추가될 때 마다 차원이 하나씩 추가된다. 
> <center>직선 -> 평면 -> 초평면 </center>

그러면 은닉계층과 회귀의 관계는 다음과 같다. 

```
선형적으로 분포되지 않는 입력 -> 선형적으로 분포하는 은닉 계층(특징) -> 선형 회귀 입력 공간을 기준으로 보면 회귀 곡선이 된다.
```
# 출처

- 그림 1.1 : https://untitledtblog.tistory.com/141
- 그림 1.2 : 패스트 캠퍼스 딥러닝/인공지능 올인원 패키지 Online 강의 자료.
- 그림 1.3 : https://en.wikipedia.org/wiki/Sigmoid_function
- 그림 1.4 : https://walkingwithus.tistory.com/606