{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-08-07.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAOu6jrvBuf-",
        "outputId": "5b4fee8d-4fd1-4457-dea0-dc1f6097476b"
      },
      "source": [
        "!pip install unidecode # unidecode 패키지 다운로드\n",
        "\n",
        "import unidecode\n",
        "\n",
        "# Figure1 참고\n",
        "# 이걸 실행하면 링크와 함께 무슨 authentication code를 넣으라고 칸이 뜬다.\n",
        "# 링크 누르고 계정 엑세스 하는걸 허용해 준 다음 링크를 복사해서 칸에 입력하면 된다.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2FRl9DnBX0G"
      },
      "source": [
        "# tf_data의 사용법에 대해서 살펴보자. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfUZTPVeBqbf"
      },
      "source": [
        "## 데이터 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2kRtfDpB-Ud",
        "outputId": "3b82e5a3-88c0-439c-fa00-ecd437e51da8"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "trainPath = '/content/drive/My Drive/Colab Notebooks/data/cifar/train/*.png'\n",
        "testPath = '/content/drive/My Drive/Colab Notebooks/data/cifar/test/*.png' \n",
        "trainDataPaths = glob(trainPath)\n",
        "testDataPaths = glob(testPath)\n",
        "\n",
        "print(\"[*] 데이터 경로 확인\")\n",
        "print(f\"   [-] train data : {len(trainDataPaths)}\")\n",
        "print(f\"   [-] train data sample: {trainDataPaths[0]}\")\n",
        "print(f\"   [-] test data : {len(testDataPaths)}\")\n",
        "print(f\"   [-] train data sample: {testDataPaths[0]}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] 데이터 경로 확인\n",
            "   [-] train data : 50000\n",
            "   [-] train data sample: /content/drive/My Drive/Colab Notebooks/data/cifar/train/7202_frog.png\n",
            "   [-] test data : 10000\n",
            "   [-] train data sample: /content/drive/My Drive/Colab Notebooks/data/cifar/test/1524_cat.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMMkU76CDxTZ"
      },
      "source": [
        "## 이미지 데이터 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "2-1eJIvXD-5a",
        "outputId": "cd321951-711b-4011-dcb9-78cca6dbde91"
      },
      "source": [
        "testPath = trainDataPaths[0]\n",
        "gfile = tf.io.read_file(testPath)\n",
        "image = tf.io.decode_image(gfile)\n",
        "print(f\"[*] 샘플 데이터 이미지 확인\")\n",
        "print(f\"    [-] shape : {image.shape}\")\n",
        "plt.title(os.path.basename(testPath))\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[*] 샘플 데이터 이미지 확인\n",
            "    [-] shape : (32, 32, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbyebMUFDaS"
      },
      "source": [
        "## tf_data 사용을 위한 전처리 단계 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U_5T2YhGmNe"
      },
      "source": [
        "def read_image(path):\n",
        "  gfile = tf.io.read_file(path)\n",
        "  image = tf.io.decode_image(gfile)\n",
        "  return image\n",
        "\n",
        "def get_label(path):\n",
        "  return path.split('_')[-1].replace('.png', '')\n",
        "\n",
        "label_name = [get_label(path) for path in trainDataPaths]     # trainDataPaths에 저장되어있는 경로들에서 라벨을 추출\n",
        "class_names = np.unique(label_name)                           # 중복 제거를 통해서 class_names 생성\n",
        "\n",
        "def onehot_encoding(label):\n",
        "  return np.array(class_name == label, np.uint8)\n",
        "\n",
        "def read_image_label(path, label):\n",
        "  gfile = tf.io.read_file(path)\n",
        "  image = tf.io.decode_image(gfile)\n",
        "  return image, label\n",
        "\n",
        "def get_label_tensor(path):\n",
        "  fname = tf.strings.split(path, '_')[-1]\n",
        "  cls_name = tf.strings.regex_replace(fname, '.png', '')\n",
        "  onehot_encoding = tf.cast(class_name == cls_name, tf.uint8)\n",
        "  return onehot_encoding"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfnGoeCHLzDE"
      },
      "source": [
        "우리는 라벨값을 얻는데 두가지 함수를 사용하였다. \n",
        " - `split()` : python 내장 함수 + `np.array()` : numpy 내장 함수\n",
        " - `tf.string.split()` : tensorflow 내장 함수 + `tf.cast` : tensorflow 내장 함수\n",
        "\n",
        "어떤 방법이 더 좋은 방법일까? \n",
        "\n",
        "검색을 해보니 일단 `tf.string.split()` 함수는 정규식도 지원하는 것으로 보인다. 이 함수의 입력 인자에 대한 설명은 다음과 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYN-rD6OLwrJ"
      },
      "source": [
        "`tf.strings.split(input, sep=None, maxsplit=-1, name=None)`\n",
        " - `input` : A string Tensor of rank N, the strings to split. If rank(input) is not known statically, then it is assumed to be 1.\n",
        " - `sep` : 0-D string Tensor, the delimiter string.\n",
        " - `maxsplit` : An int. If maxsplit > 0, limit of the split of the result.\n",
        " - `name` : A name for the operation (optional)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmKUok06O29e"
      },
      "source": [
        "즉 `tf.string.split` 함수가 `split`보다 조금 더 많은 기능을 지원한다. `np.array()`와 `tf.cast()`는 크게 차이가 없어 보인다. 자신에게 더 맞는 아무거나 사용하면 될 것 같다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKrUnf9zFnz_"
      },
      "source": [
        "### tf.data.Dataset.from_tensor_slices(filenames)\n",
        "\n",
        "파일들을 불러들여 tensor를 생성하는 함수."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8TjHqk-GuzD"
      },
      "source": [
        "batch_size = 8\n",
        "dataset = tf.data.Dataset.from_tensor_slices(trainDataPaths)    # 해당 경로들을 모두 텐서로 생성\n",
        "dataset = dataset.map(read_image)                               # 각 경로들을 읽어서 이미지로 변환\n",
        "dataset = dataset.batch(batch_size=batch_size)                  # 읽은 모든 데이터를 batch_size 만큼으로 분리\n",
        "tf_image = next(iter(dataset))\n",
        "print(f\"생성한 dataset shape : {tf_image.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "391KbE4ZGCT-"
      },
      "source": [
        "### shuffle \n",
        "\n",
        "불러들인 데이터의 순서을 무작위로 섞는다. 단 이 메소드를 사용할 경우 시간이 오래걸린다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqoNR4r8HuU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729c6df4-694c-4897-a442-9aa21df23ea7"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(trainDataPaths)    # 해당 경로들을 모두 텐서로 생성\n",
        "dataset = dataset.map(read_image)                               # 각 경로들을 읽어서 이미지로 변환\n",
        "dataset = dataset.batch(batch_size=batch_size)                  # 읽은 모든 데이터를 batch_size 만큼으로 분리\n",
        "dataset = dataset.shuffle(buffer_size = len(trainDataPaths))    # 버퍼 사이즈 만큼의 데이터를 랜덤으로 섞는다.\n",
        "tf_image = next(iter(dataset))\n",
        "print(f\"생성한 dataset shape : {tf_image.shape}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "생성한 dataset shape : (8, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvWKgDglITN7"
      },
      "source": [
        "### repeat\n",
        "\n",
        "데이터 셋을 반복한다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOttyGHAJj7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edc2df1f-9fd7-41f4-a72f-a603246d4748"
      },
      "source": [
        "label_names = [get_label(path) for path in trainDataPaths]     # trainDataPaths에 저장되어있는 경로들에서 라벨을 추출\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((trainDataPaths, label_names))\n",
        "dataset = dataset.map(read_image_label)\n",
        "dataset = dataset.batch(batch_size = batch_size)\n",
        "dataset = dataset.repeat()\n",
        "image, label = next(iter(dataset))\n",
        "print(f'image shape : {image.shape}')\n",
        "print(f'label shape : {label.shape}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image shape : (8, 32, 32, 3)\n",
            "label shape : (8,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT9deloePgRR"
      },
      "source": [
        "## fit with tf.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t69C8v6GPvDP"
      },
      "source": [
        "### 사용할 함수들 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SH7EWt8LP-uD"
      },
      "source": [
        "def get_class_name(path):\n",
        "  fname = tf.strings.split(path, '_')[-1]\n",
        "  lbl_name = tf.strings.regex_replace(fname, '.png', '')\n",
        "  return lbl_name\n",
        "\n",
        "class_names = [get_class_name(path) for path in trainDataPaths]\n",
        "classes = tf.unique(class_names).y.numpy()\n",
        "\n",
        "def onehot_encoding_function(path):\n",
        "  onehot_encoding = tf.cast(classes == get_class_name(path), tf.uint8)\n",
        "  return onehot_encoding\n",
        "\n",
        "def read_dataset(path):\n",
        "  gfile = tf.io.read_file(path)\n",
        "  image = tf.io.decode_image(gfile)\n",
        "\n",
        "  image = tf.cast(image, tf.float32) / 255.\n",
        "\n",
        "  label = onehot_encoding_function(path)\n",
        "\n",
        "  return image, label\n",
        "\n",
        "def image_preprocess(image, label):\n",
        "  image = tf.image.random_flip_left_right(image)      # 이미지를 랜덤으로 왼쪽 오른쪽으로 뒤집는다.\n",
        "  image = tf.image.random_flip_up_down(image)         # 이미지를 랜덤으로 위 아래로 뒤집는다.\n",
        "  return image, label"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ7_S0N-SaQg"
      },
      "source": [
        "### 하이퍼 파라미터 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3XCmkGOSczs"
      },
      "source": [
        "num_epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "learning_rate = 0.001\n",
        "dropout_rate = 0.5\n",
        "\n",
        "input_shape = (32,32,3)\n",
        "num_classes = 10"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWVvNwMGR0_F"
      },
      "source": [
        "### 데이터셋 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es4TTwPkR5Zx"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(trainDataPaths)\n",
        "train_dataset = train_dataset.map(read_dataset)\n",
        "train_dataset = train_dataset.map(image_preprocess)\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(testDataPaths)\n",
        "test_dataset = test_dataset.map(read_dataset)\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.repeat()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEc6b-H9ie5V"
      },
      "source": [
        "### 모델 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMcWOf3giixq"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "inputs = layers.Input(input_shape)\n",
        "net = layers.Conv2D(32, (3,3), padding = 'SAME')(inputs)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.Conv2D(32, (3,3), padding = 'SAME')(net)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.MaxPooling2D(pool_size=(2,2))(net)\n",
        "net = layers.Dropout(dropout_rate)(net)\n",
        "\n",
        "net = layers.Conv2D(64, (3,3), padding = 'SAME')(net)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.Conv2D(64, (3,3), padding = 'SAME')(net)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.MaxPooling2D(pool_size = (2,2))(net)\n",
        "net = layers.Dropout(dropout_rate)(net)\n",
        "\n",
        "net = layers.Flatten()(net)\n",
        "net = layers.Dense(512)(net)\n",
        "net = layers.Activation('relu')(net)\n",
        "net = layers.Dropout(dropout_rate)(net)\n",
        "net = layers.Dense(num_classes)(net)\n",
        "net = layers.Activation('softmax')(net)\n",
        "\n",
        "model = tf.keras.Model(inputs = inputs, outputs = net, name = 'Basic_CNN')\n",
        "\n",
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate),\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb2X6Xu7nW1b",
        "outputId": "6f42dd9f-8399-49b2-9a52-101a43a2156b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Basic_CNN\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               2097664   \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 2,168,362\n",
            "Trainable params: 2,168,362\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwMrsEqim8M8",
        "outputId": "d2354e31-eac0-404e-9a6e-5245981cd31e"
      },
      "source": [
        "model.fit_generator(\n",
        "    train_dataset,\n",
        "    steps_per_epoch = len(trainDataPaths) // batch_size,\n",
        "    validation_data = test_dataset,\n",
        "    validation_steps = len(testDataPaths) // batch_size,\n",
        "    epochs = num_epochs\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1562/1562 [==============================] - 1680s 1s/step - loss: 1.7099 - accuracy: 0.3550 - val_loss: 1.3756 - val_accuracy: 0.4867\n",
            "Epoch 2/10\n",
            "1562/1562 [==============================] - 362s 232ms/step - loss: 1.4160 - accuracy: 0.4783 - val_loss: 1.2161 - val_accuracy: 0.5551\n",
            "Epoch 3/10\n",
            "1562/1562 [==============================] - 363s 233ms/step - loss: 1.3070 - accuracy: 0.5241 - val_loss: 1.1166 - val_accuracy: 0.5984\n",
            "Epoch 4/10\n",
            "1562/1562 [==============================] - 364s 233ms/step - loss: 1.2323 - accuracy: 0.5543 - val_loss: 1.0851 - val_accuracy: 0.6103\n",
            "Epoch 5/10\n",
            "1562/1562 [==============================] - 362s 232ms/step - loss: 1.1915 - accuracy: 0.5711 - val_loss: 1.0540 - val_accuracy: 0.6231\n",
            "Epoch 6/10\n",
            "1562/1562 [==============================] - 362s 232ms/step - loss: 1.1545 - accuracy: 0.5885 - val_loss: 1.0250 - val_accuracy: 0.6363\n",
            "Epoch 7/10\n",
            "1562/1562 [==============================] - 363s 232ms/step - loss: 1.1279 - accuracy: 0.5980 - val_loss: 0.9693 - val_accuracy: 0.6610\n",
            "Epoch 8/10\n",
            "1562/1562 [==============================] - 363s 232ms/step - loss: 1.1076 - accuracy: 0.6023 - val_loss: 0.9711 - val_accuracy: 0.6574\n",
            "Epoch 9/10\n",
            "1562/1562 [==============================] - 364s 233ms/step - loss: 1.0880 - accuracy: 0.6112 - val_loss: 1.0348 - val_accuracy: 0.6314\n",
            "Epoch 10/10\n",
            "1562/1562 [==============================] - 367s 235ms/step - loss: 1.0754 - accuracy: 0.6154 - val_loss: 0.9780 - val_accuracy: 0.6561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f35addd5650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8vernbTnPOp"
      },
      "source": [
        "# callbacks에 대해서 알아보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_r65Tixnd4W"
      },
      "source": [
        "모델을 훈련시키는 일은 대체적으로 오랜 시간이 걸린다. AI 개발자가 훈련을 따다 끝날때 까지 가만히 앉아서 잘 학습이 되기를 기도하는 것은 너무 불안한 일이다. 그래서 Tensorflow는 모델을 훈련시키는 동안 어떤 이벤트들이 발생하면 개발자가 원하는 동작을 수행할 수 있는 방법을 제공하고 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXo3BL-HoV5R"
      },
      "source": [
        "## LambdaCallback\n",
        "\n",
        "Callback을 새롭게 만들어 쓰고는 싶은데 전체 클래스를 새로 만들기에는 부담스러울 때가 존재한다. 이때 Callback을 함수 형태로 전달할 수 있게 해주는 것이 LambdaCallback이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf0ya8FOpeYc"
      },
      "source": [
        "# https://www.tensorflow.org/tensorboard/r2/image_summaries\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import sklearn.metrics\n",
        "import itertools\n",
        "import io\n",
        "logdir = os.path.join('/content/drive/My Drive/Colab Notebooks/logs', datetime.now().strftime('%Y%m%d-%H%M%S'))\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')\n",
        "\n",
        "def plot_to_image(figure):\n",
        "    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "    # Save the plot to a PNG in memory.\n",
        "    buf = io.BytesIO()\n",
        "    plt.savefig(buf, format='png')\n",
        "    # Closing the figure prevents it from being displayed directly inside\n",
        "    # the notebook.\n",
        "    plt.close(figure)\n",
        "    buf.seek(0)\n",
        "    # Convert PNG buffer to TF image\n",
        "    image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    # Add the batch dimension\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "\n",
        "    Args:\n",
        "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "    class_names (array, shape = [n]): String names of the integer classes\n",
        "    \"\"\"\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Normalize the confusion matrix.\n",
        "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "    # Use white text if squares are dark; otherwise black.\n",
        "    threshold = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return figure"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRGj3YA5pgFT"
      },
      "source": [
        "test_images, test_labels = next(iter(test_dataset))  # Confusion Matrix 그릴 때 필요한 Test Image\n",
        "\n",
        "\n",
        "def log_confusion_matrix(epoch, logs):\n",
        "    # Use the model to predict the values from the validation dataset.\n",
        "    test_pred_raw = model.predict(test_images)\n",
        "    test_pred = np.argmax(test_pred_raw, axis=1)\n",
        "\n",
        "    # Calculate the confusion matrix.\n",
        "    cm = sklearn.metrics.confusion_matrix(test_labels, test_pred)\n",
        "    # Log the confusion matrix as an image summary.\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "\n",
        "    # Log the confusion matrix as an image summary.\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKklaEQAplZ9"
      },
      "source": [
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oomQLH_sppmD"
      },
      "source": [
        " model.fit_generator(\n",
        "    train_dataset, \n",
        "    steps_per_epoch = len(trainDataPaths) // batch_size,\n",
        "    validation_data = test_dataset,\n",
        "    validation_steps = len(testDataPaths) // batch_size,\n",
        "    epochs = num_epochs,\n",
        "    callbacks=[cm_callback]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}